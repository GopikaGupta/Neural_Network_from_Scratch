{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration1,loss:36.0\n",
      "iteration2,loss:34.00056099999998\n",
      "iteration3,loss:32.111851559824\n",
      "iteration4,loss:30.327748437088754\n",
      "iteration5,loss:28.642466747934694\n",
      "iteration6,loss:27.0505412770096\n",
      "iteration7,loss:25.546808819642305\n",
      "iteration8,loss:24.12639149920705\n",
      "iteration9,loss:22.784681005827146\n",
      "iteration10,loss:21.517323705539766\n",
      "iteration11,loss:20.32020657185342\n",
      "iteration12,loss:19.189443894283603\n",
      "iteration13,loss:18.12136472096045\n",
      "iteration14,loss:17.11250099477067\n",
      "iteration15,loss:16.159576344735388\n",
      "iteration16,loss:15.259495496439483\n",
      "iteration17,loss:14.409334267326745\n",
      "iteration18,loss:13.606330114562574\n",
      "iteration19,loss:12.847873204949495\n",
      "iteration20,loss:12.131497978065813\n",
      "iteration21,loss:11.45487517538972\n",
      "iteration22,loss:10.815804309675135\n",
      "iteration23,loss:10.212206550266686\n",
      "iteration24,loss:9.642118001383809\n",
      "iteration25,loss:9.103683351672142\n",
      "iteration26,loss:8.595149874519082\n",
      "iteration27,loss:8.114861759762302\n",
      "iteration28,loss:7.661254758489921\n",
      "iteration29,loss:7.232851123641462\n",
      "iteration30,loss:6.828254830073704\n",
      "iteration31,loss:6.446147058657477\n",
      "iteration32,loss:6.085281929823893\n",
      "iteration33,loss:5.744482472783582\n",
      "iteration34,loss:5.4226368174033\n",
      "iteration35,loss:5.118694596443057\n",
      "iteration36,loss:4.831663546535971\n",
      "iteration37,loss:4.560606296934514\n",
      "iteration38,loss:4.304637335653096\n",
      "iteration39,loss:4.0629201432095075\n",
      "iteration40,loss:3.8346644847087927\n",
      "iteration41,loss:3.619123851524306\n",
      "iteration42,loss:3.4155930443136433\n",
      "iteration43,loss:3.223405889563422\n",
      "iteration44,loss:3.04193308228792\n",
      "iteration45,loss:2.8705801479139086\n",
      "iteration46,loss:2.7087855167687613\n",
      "iteration47,loss:2.5560187049524656\n",
      "iteration48,loss:2.411778595717645\n",
      "iteration49,loss:2.275591815806157\n",
      "iteration50,loss:2.147011201497429\n",
      "iteration51,loss:2.025614349413373\n",
      "iteration52,loss:1.9110022473982924\n",
      "iteration53,loss:1.802797981050815\n",
      "iteration54,loss:1.700645511729126\n",
      "iteration55,loss:1.6042085220815003\n",
      "iteration56,loss:1.5131693253722753\n",
      "iteration57,loss:1.4272278350792575\n",
      "iteration58,loss:1.3461005914333108\n",
      "iteration59,loss:1.2695198417546356\n",
      "iteration60,loss:1.197232671614038\n",
      "iteration61,loss:1.1290001840115549\n",
      "iteration62,loss:1.0645967239199172\n",
      "iteration63,loss:1.0038091456867733\n",
      "iteration64,loss:0.9464361209280554\n",
      "iteration65,loss:0.892287484675589\n",
      "iteration66,loss:0.8411836176656193\n",
      "iteration67,loss:0.792954862771644\n",
      "iteration68,loss:0.7474409736952008\n",
      "iteration69,loss:0.7044905941324638\n",
      "iteration70,loss:0.6639607657329202\n",
      "iteration71,loss:0.6257164632593959\n",
      "iteration72,loss:0.5896301554465692\n",
      "iteration73,loss:0.5555813901381053\n",
      "iteration74,loss:0.5234564023609822\n",
      "iteration75,loss:0.4931477440696606\n",
      "iteration76,loss:0.46455393436275205\n",
      "iteration77,loss:0.43757912904098983\n",
      "iteration78,loss:0.4121328084377696\n",
      "iteration79,loss:0.38812948251257157\n",
      "iteration80,loss:0.3654884122533453\n",
      "iteration81,loss:0.34413334648663063\n",
      "iteration82,loss:0.3239922732439719\n",
      "iteration83,loss:0.3049971848802046\n",
      "iteration84,loss:0.2870838561836533\n",
      "iteration85,loss:0.27019163476022456\n",
      "iteration86,loss:0.2542632430130821\n",
      "iteration87,loss:0.23924459107703214\n",
      "iteration88,loss:0.2250846001021594\n",
      "iteration89,loss:0.21173503531470556\n",
      "iteration90,loss:0.19915034831478542\n",
      "iteration91,loss:0.18728752810037264\n",
      "iteration92,loss:0.17610596033521148\n",
      "iteration93,loss:0.16556729440495108\n",
      "iteration94,loss:0.15563531783097934\n",
      "iteration95,loss:0.14627583763520674\n",
      "iteration96,loss:0.13745656827154357\n",
      "iteration97,loss:0.12914702576102272\n",
      "iteration98,loss:0.12131842768759432\n",
      "iteration99,loss:0.11394359873056079\n",
      "iteration100,loss:0.10699688142752571\n",
      "iteration101,loss:0.10045405187864993\n",
      "iteration102,loss:0.09429224011897672\n",
      "iteration103,loss:0.08848985490069715\n",
      "iteration104,loss:0.0830265126414905\n",
      "iteration105,loss:0.07788297030853633\n",
      "iteration106,loss:0.07304106202054561\n",
      "iteration107,loss:0.06848363916216811\n",
      "iteration108,loss:0.06419451381651868\n",
      "iteration109,loss:0.060158405332273464\n",
      "iteration110,loss:0.05636088985195621\n",
      "iteration111,loss:0.0527883526376021\n",
      "iteration112,loss:0.049427943039043416\n",
      "iteration113,loss:0.04626753195861983\n",
      "iteration114,loss:0.04329567167418518\n",
      "iteration115,loss:0.040501557889930546\n",
      "iteration116,loss:0.037874993891746096\n",
      "iteration117,loss:0.035406356690661604\n",
      "iteration118,loss:0.03308656504434629\n",
      "iteration119,loss:0.03090704925272208\n",
      "iteration120,loss:0.028859722629499835\n",
      "iteration121,loss:0.026936954556872544\n",
      "iteration122,loss:0.025131545035725168\n",
      "iteration123,loss:0.02343670064857093\n",
      "iteration124,loss:0.021846011856998672\n",
      "iteration125,loss:0.020353431559741633\n",
      "iteration126,loss:0.018953254841561486\n",
      "iteration127,loss:0.017640099847005403\n",
      "iteration128,loss:0.016408889716735015\n",
      "iteration129,loss:0.015254835527576963\n",
      "iteration130,loss:0.014173420180695043\n",
      "iteration131,loss:0.01316038318536159\n",
      "iteration132,loss:0.012211706288708345\n",
      "iteration133,loss:0.011323599904583986\n",
      "iteration134,loss:0.010492490297236654\n",
      "iteration135,loss:0.009715007477988955\n",
      "iteration136,loss:0.008987973775388777\n",
      "iteration137,loss:0.008308393041502906\n",
      "iteration138,loss:0.0076734404590885616\n",
      "iteration139,loss:0.007080452916327365\n",
      "iteration140,loss:0.0065269199176497756\n",
      "iteration141,loss:0.006010475000919615\n",
      "iteration142,loss:0.005528887632895362\n",
      "iteration143,loss:0.005080055556434269\n",
      "iteration144,loss:0.004661997564380944\n",
      "iteration145,loss:0.00427284667646144\n",
      "iteration146,loss:0.00391084369682064\n",
      "iteration147,loss:0.0035743311310751404\n",
      "iteration148,loss:0.0032617474429249256\n",
      "iteration149,loss:0.002971621631471298\n",
      "iteration150,loss:0.002702568111432963\n",
      "iteration151,loss:0.002453281879438062\n",
      "iteration152,loss:0.002222533950501296\n",
      "iteration153,loss:0.0020091670496757987\n",
      "iteration154,loss:0.0018120915447002953\n",
      "iteration155,loss:0.001630281606248008\n",
      "iteration156,loss:0.0014627715831254343\n",
      "iteration157,loss:0.0013086525804702773\n",
      "iteration158,loss:0.001167069229659544\n",
      "iteration159,loss:0.0010372166392651203\n",
      "iteration160,loss:0.0009183375169845854\n",
      "iteration161,loss:0.0008097194530330593\n",
      "iteration162,loss:0.0007106923560098516\n",
      "iteration163,loss:0.0006206260327516026\n",
      "iteration164,loss:0.000538927904153975\n",
      "iteration165,loss:0.0004650408493890596\n",
      "iteration166,loss:0.0003984411713651027\n",
      "iteration167,loss:0.00033863667667246395\n",
      "iteration168,loss:0.00028516486363405714\n",
      "iteration169,loss:0.00023759121243277333\n",
      "iteration170,loss:0.0001955075716227617\n",
      "iteration171,loss:0.00015853063564759374\n",
      "iteration172,loss:0.0001263005082864424\n",
      "iteration173,loss:9.847934723159293e-05\n",
      "iteration174,loss:7.47500852668501e-05\n",
      "iteration175,loss:5.481522376792748e-05\n",
      "iteration176,loss:3.839569448367886e-05\n",
      "iteration177,loss:2.5229785781274425e-05\n",
      "iteration178,loss:1.5072129750779517e-05\n",
      "iteration179,loss:7.692746764676367e-06\n",
      "iteration180,loss:2.876144277084615e-06\n",
      "iteration181,loss:4.204668263503696e-07\n",
      "iteration182,loss:0.0\n",
      "iteration183,loss:0.0\n",
      "iteration184,loss:0.0\n",
      "iteration185,loss:0.0\n",
      "iteration186,loss:0.0\n",
      "iteration187,loss:0.0\n",
      "iteration188,loss:0.0\n",
      "iteration189,loss:0.0\n",
      "iteration190,loss:0.0\n",
      "iteration191,loss:0.0\n",
      "iteration192,loss:0.0\n",
      "iteration193,loss:0.0\n",
      "iteration194,loss:0.0\n",
      "iteration195,loss:0.0\n",
      "iteration196,loss:0.0\n",
      "iteration197,loss:0.0\n",
      "iteration198,loss:0.0\n",
      "iteration199,loss:0.0\n",
      "iteration200,loss:0.0\n",
      "final weights: [-3.41566927 -0.16866147  0.7529922 ]\n",
      "final bias: 0.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "### Initial parameters\n",
    "weights=np.array([-3.0,-1.0,2.0])\n",
    "bias=1.0\n",
    "inputs=np.array([1.0,-2.0,3.0])\n",
    "target_output=0.\n",
    "learning_rate=0.001\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return(np.maximum(0,x))\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x>0,1.,0.)\n",
    "\n",
    "for iterations in range(200):\n",
    "    # forward pass\n",
    "    linear_output=np.dot(weights,inputs)+bias\n",
    "    output=relu(linear_output)\n",
    "    loss=(output-target_output)**2\n",
    "\n",
    "\n",
    "    #backward pass\n",
    "    dloss_output=2*(output-target_output)\n",
    "    doutput_dlinear=relu_derivative(linear_output)\n",
    "    dlinear_dweights=inputs\n",
    "    dlinear_dbias=1.0\n",
    "\n",
    "    dloss_dlinear=dloss_output*doutput_dlinear\n",
    "    dloss_dweights=dloss_dlinear*dlinear_dweights\n",
    "    dloss_dbias=dloss_dlinear*dlinear_dbias\n",
    "\n",
    "\n",
    "    ##update weights and biases\n",
    "    weights-=learning_rate*dloss_dweights\n",
    "    bias-=learning_rate*dlinear_dbias\n",
    "\n",
    "\n",
    "    ## print the loss for this iteration\n",
    "    print(f\"iteration{iterations+1},loss:{loss}\")\n",
    "\n",
    "\n",
    "print('final weights:',weights)\n",
    "print('final bias:',bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
